# -*- coding: utf-8 -*-
"""MiniChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ifTCdaIXZ3UXc8hJ4k6ti_mDwV_E5pOe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_json("/content/chatbot.json")

df.head()

data_dict = {
      "tag" : [],
      "patterns" : [],
      "responses" : []
  }
def createDataframe():
  for index, data in df.iterrows():
    dic = data.to_dict()
    # print(index,dic["intents"])
    new_dic = dic["intents"]
    for key, data in new_dic.items():
      if key != "context_set":
        data_dict[key].append(data)

createDataframe()

df1 = pd.DataFrame(data_dict, columns=["tag", "patterns" ,"responses"])

df1.shape

df1.head()

df2 = df1.drop(columns=["responses"])

df2

final_dict = {
    "tag" :[],
    "patterns" : []
}
for index, value in df2.iterrows():
  tag = value[0]
  patterns = value[1]

  for i in patterns:
    final_dict["tag"].append(tag)
    final_dict["patterns"].append(i)

df3 = pd.DataFrame(final_dict, columns=["tag" ,"patterns"])

df3

import nltk
nltk.download("punkt")
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

## Data Preprocessing
## 1. convert to lowercase
## 2. remove stop words and special characters
## 3. stemming(conversion of similar words to their root words : dance, dancing , danced == dance)

from nltk.corpus import stopwords
nltk.download("stopwords")
import string

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()



## function for data preprocessing of the comment_text column
def preprocessing(text):
  text = text.lower()
  text = nltk.word_tokenize(text)
  y = []
  for i in text:
    if i.isalnum():
      y.append(i)
  text = list(y)
  y.clear()

  # for i in text:
  #   if i not in stopwords.words("english"):
  #     y.append(i)

  # text = list(y)
  # y.clear()

  for i in text:
    if i not in string.punctuation:
      y.append(i)

  text = list(y)
  y.clear()

  for i in text:
    y.append(ps.stem(i))


  return " ".join(y)

import sys
sys.setrecursionlimit(10**6)

df3["patterns"] = df3["patterns"].apply(preprocessing)

df3

from wordcloud import WordCloud

# whole_text = (new_df[new_df["toxic"] == 1]["transformed_text"]).to_list()
# text_data = " ".join(whole_text)

# # Split the text into words (assuming space-separated words)
# word_list = text_data.split()

# # Create a WordCloud object
# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

# # Display the generated word cloud using matplotlib
# plt.figure(figsize=(10, 5))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')  # Turn off axis labels
# plt.show()

import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = CountVectorizer()
tfid = TfidfVectorizer(max_features=100) #1000

le = LabelEncoder()

y = le.fit_transform(df3["tag"])

label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))

X = tfid.fit_transform(df3["patterns"]).toarray()

# query = X[0].reshape(1, -1)
# query.shape

X.shape

y.shape

y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2,test_size=0.2)

from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score
gb = GaussianNB()
bb = BernoulliNB()
mb = MultinomialNB()

mb.fit(X_train, y_train)

y_pred = mb.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
mnb = MultinomialNB()
dtc = DecisionTreeClassifier()
lrc = LogisticRegression(solver='saga', penalty='l2')
rfc = RandomForestClassifier(n_estimators=100, random_state=42)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=200, random_state=42)
xgb = XGBClassifier(n_estimators=200,random_state=42)

clfs = {
     'SVC' : svc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'xgb':xgb
}

rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)
accuracy_score(y_test, y_pred)

bc.fit(X_train, y_train)
y_pred = bc.predict(X_test)
accuracy_score(y_test, y_pred)

svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
accuracy_score(y_test, y_pred)

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense

X_train

model = Sequential()

model.add(Dense(32, activation="relu", input_dim=100))
model.add(Dense(64, activation="relu"))
model.add(Dense(128, activation="relu"))

model.add(Dense(38, activation="softmax"))

model.summary()

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=200, validation_split=0.2 )

import matplotlib.pyplot as plt

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])

y_pred = model.predict(X_test)

test = tfid.transform(["your name"])

test.A

arr = model.predict(test.A)

arr

arr = arr.reshape(38)

arr

j = 0
mx = -1
idx = -1
for i in arr:
  if i > mx:
    mx = i
    idx = j
  j = j + 1

idx

cls = None
for key, value in label_mapping.items():
  if value == idx:
    cls = key
    break

cls

df1[df1["tag"] == cls]["responses"]

import pandas as pd
response = None
# Assuming df1 is your DataFrame and cls is the class you're interested in
filtered_responses = df1[df1["tag"] == cls]["responses"]

if not filtered_responses.empty:
    first_response = filtered_responses.iloc[0]
    response = first_response
else:
    print("No responses found for the specified class.")

size = len(response)

from numpy import random

rd = np.random.randint(0,size)

lst = response

lst[rd]

df2

def get_response(input_text):
    # Implement your chatbot logic here
    # This function should take user input as input_text and return a response
    # For demonstration purposes, let's echo back the input text
    test = tfid.transform([input_text])
    arr = model.predict(test.A)
    arr = arr.reshape(38)
    j = 0
    mx = -1
    idx = -1
    for i in arr:
      if i > mx:
        mx = i
        idx = j
      j = j + 1

    cls = None
    for key, value in label_mapping.items():
      if value == idx:
        cls = key
        break
    response = None
    # Assuming df1 is your DataFrame and cls is the class you're interested in
    filtered_responses = df1[df1["tag"] == cls]["responses"]

    if not filtered_responses.empty:
        first_response = filtered_responses.iloc[0]
        response = first_response
    else:
        print("No responses found for the specified class.")

    size = len(response)
    rd = np.random.randint(0,size)
    lst = response

    return "Bot: " + lst[rd]


def main():
    print("Welcome to the Chatbot!")
    print("You can start chatting. Type 'exit' to end the conversation.")

    while True:
        user_input = input("You: ")

        if user_input.lower() == 'exit':
            print("Goodbye!")
            break

        bot_response = get_response(user_input)
        print(bot_response)


if __name__ == "__main__":
    main()

